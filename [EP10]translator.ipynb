{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 번역기를 만들어보자\n",
    "\n",
    "<<목차>>\n",
    "\n",
    "1) 정제, 정규화, 전처리 (영어, 프랑스어 모두!)  \n",
    "\n",
    "2) 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요.\n",
    "\n",
    "3) 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요.\n",
    "\n",
    "4) 임베딩 층(Embedding layer) 사용하기\n",
    "\n",
    "5) 모델 구현하기\n",
    "\n",
    "6) 모델 평가하기\n",
    "\n",
    "7) 루브릭\n",
    "\n",
    "8) 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.manythings.org/anki/\n",
    "\n",
    "사용할 데이터는  프랑스어와 영어의 병렬 코퍼스인 fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 정제, 정규화, 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 :  197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173979</th>\n",
       "      <td>My painting is starting to look pretty cool.</td>\n",
       "      <td>Mon tableau commence à avoir de la gueule.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97255</th>\n",
       "      <td>Not all of them are present.</td>\n",
       "      <td>Ils ne sont pas tous présents.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90416</th>\n",
       "      <td>There is a man at the door.</td>\n",
       "      <td>Il y a un homme à la porte.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88665</th>\n",
       "      <td>I'm living with my parents.</td>\n",
       "      <td>Je vis avec mes parents.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169890</th>\n",
       "      <td>Tom recommends that you not do that again.</td>\n",
       "      <td>Tom vous conseille de ne plus recommencer.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 eng  \\\n",
       "173979  My painting is starting to look pretty cool.   \n",
       "97255                   Not all of them are present.   \n",
       "90416                    There is a man at the door.   \n",
       "88665                    I'm living with my parents.   \n",
       "169890    Tom recommends that you not do that again.   \n",
       "\n",
       "                                               fra  \\\n",
       "173979  Mon tableau commence à avoir de la gueule.   \n",
       "97255               Ils ne sont pas tous présents.   \n",
       "90416                  Il y a un homme à la porte.   \n",
       "88665                     Je vis avec mes parents.   \n",
       "169890  Tom vous conseille de ne plus recommencer.   \n",
       "\n",
       "                                                       cc  \n",
       "173979  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "97255   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "90416   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "88665   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "169890  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/june/data/translator_seq2seq/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print(\"전체 샘플의 수 : \", len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35656</th>\n",
       "      <td>I can't do it alone.</td>\n",
       "      <td>Je ne peux le faire seul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11510</th>\n",
       "      <td>No one laughed.</td>\n",
       "      <td>Personne n'a ri.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33662</th>\n",
       "      <td>Why don't you quit?</td>\n",
       "      <td>Pourquoi n'arrêtez-vous pas ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49835</th>\n",
       "      <td>I know how this works.</td>\n",
       "      <td>Je sais comment cela fonctionne.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16913</th>\n",
       "      <td>What's the idea?</td>\n",
       "      <td>Quelle est l'idée ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eng                               fra\n",
       "35656    I can't do it alone.         Je ne peux le faire seul.\n",
       "11510         No one laughed.                  Personne n'a ri.\n",
       "33662     Why don't you quit?     Pourquoi n'arrêtez-vous pas ?\n",
       "49835  I know how this works.  Je sais comment cela fonctionne.\n",
       "16913        What's the idea?               Quelle est l'idée ?"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_decoder(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26411        [this, is, his, fault, .]\n",
       "41073    [do, you, know, his, name, ?]\n",
       "34827    [don, t, do, that, to, me, .]\n",
       "27208         [we, were, impressed, .]\n",
       "21794         [what, did, you, say, ?]\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng = lines.eng.apply(lambda x : preprocess_sentence(x))\n",
    "\n",
    "lines.eng.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44789       [<start>, elle, a, parl, impoliment, ., <end>]\n",
       "18214    [<start>, il, alla, faire, des, emplettes, ., ...\n",
       "21471    [<start>, nous, pouvons, encore, gagner, ., <e...\n",
       "13434           [<start>, vous, fiez, vous, moi, ?, <end>]\n",
       "29388    [<start>, comment, pouvais, tu, savoir, ?, <end>]\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.fra = lines.fra.apply(lambda x : preprocess_sentence_decoder(x))\n",
    "\n",
    "lines.fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[33, 1], [33, 1], [33, 1], [33, 1], [908, 1]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(lines.eng)\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)\n",
    "input_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 89, 15, 2],\n",
       " [1, 458, 3, 2],\n",
       " [1, 30, 649, 15, 2],\n",
       " [1, 935, 15, 2],\n",
       " [1, 936, 15, 2]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer()\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 5987\n",
      "프랑스어 단어장의 크기 : 9313\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 9\n",
      "프랑스어 시퀀스의 최대 길이 19\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 5987\n",
      "프랑스어 단어장의 크기 : 9313\n",
      "영어 시퀀스의 최대 길이 9\n",
      "프랑스어 시퀀스의 최대 길이 19\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)   \n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<start>'\n",
    "eos_token = '<end>'\n",
    "\n",
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 9)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 19)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 19)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47000, 9)\n",
      "(47000, 19)\n",
      "(47000, 19)\n",
      "(3000, 9)\n",
      "(3000, 19)\n",
      "(3000, 19)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "\n",
    "encoder_inputs = Input(shape=(None, ), name='encoder_input')\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_size,\n",
    "                    input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_size, dropout = 0.5, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ), name='decoder_input')\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(hidden_size, dropout = 0.5, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, None, 512)    3065344     ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 512)    4768256     ['decoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " masking_4 (Masking)            (None, None, 512)    0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " masking_5 (Masking)            (None, None, 512)    0           ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  [(None, 512),        2099200     ['masking_4[0][0]']              \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, None, 512),  2099200     ['masking_5[0][0]',              \n",
      "                                 (None, 512),                     'lstm_4[0][1]',                 \n",
      "                                 (None, 512)]                     'lstm_4[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 9313)   4777569     ['lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,809,569\n",
      "Trainable params: 16,809,569\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 22:32:42.214425: W tensorflow/core/common_runtime/forward_type_inference.cc:332] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n",
      "2022-10-25 22:32:43.388914: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2022-10-25 22:32:43.730857: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1469/1469 [==============================] - 34s 19ms/step - loss: 1.2493 - acc: 0.8048 - val_loss: 1.0370 - val_acc: 0.8344\n",
      "Epoch 2/50\n",
      "1469/1469 [==============================] - 27s 18ms/step - loss: 0.9488 - acc: 0.8464 - val_loss: 0.8922 - val_acc: 0.8545\n",
      "Epoch 3/50\n",
      "1469/1469 [==============================] - 26s 18ms/step - loss: 0.8302 - acc: 0.8626 - val_loss: 0.8097 - val_acc: 0.8679\n",
      "Epoch 4/50\n",
      "1469/1469 [==============================] - 26s 18ms/step - loss: 0.7533 - acc: 0.8743 - val_loss: 0.7569 - val_acc: 0.8763\n",
      "Epoch 5/50\n",
      "1110/1469 [=====================>........] - ETA: 6s - loss: 0.6991 - acc: 0.8839"
     ]
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], \n",
    "          y=decoder_target_train, \n",
    "          validation_data = ([encoder_input_test, decoder_input_test], \n",
    "                             decoder_target_test),\n",
    "          batch_size=32, \n",
    "          epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         1532672   \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 256),             525312    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,057,984\n",
      "Trainable params: 2,057,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(embedding_size,))\n",
    "decoder_state_input_c = Input(shape=(embedding_size,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, None, 512)    4768256     ['decoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 512),  2099200     ['embedding_4[0][0]',            \n",
      "                                 (None, 512),                     'input_3[0][0]',                \n",
      "                                 (None, 512)]                     'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 9313)   4777569     ['lstm_3[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,645,025\n",
      "Trainable params: 11,645,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <start>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<start>']) and i!=fra2idx['<end>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 834ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "-----------------------------------\n",
      "입력 문장: i quit my job . \n",
      "정답 문장: j ai d missionn de mon boulot . \n",
      "번역기가 번역한 문장:  j mon mon mon ma . c\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-----------------------------------\n",
      "입력 문장: he let me go . \n",
      "정답 문장: il m a laiss partir . \n",
      "번역기가 번역한 문장:  il m m m . . . . . \n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "-----------------------------------\n",
      "입력 문장: i sat on the sofa . \n",
      "정답 문장: je me suis assis sur le canap . \n",
      "번역기가 번역한 문장:  je s e e me me on \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-----------------------------------\n",
      "입력 문장: i dropped my earring . \n",
      "정답 문장: j ai fait tomber ma boucle d oreille . \n",
      "번역기가 번역한 문장:  j me me une une un\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-----------------------------------\n",
      "입력 문장: she did pretty well . \n",
      "정답 문장: elle s en est assez bien sortie . \n",
      "번역기가 번역한 문장:  elle voulez avez bie\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,200,500,1001,2022]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.\n",
    "\n",
    "구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.\n",
    "\n",
    "\n",
    "2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.\n",
    "\n",
    "seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.\n",
    "\n",
    "\n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\n",
    "\n",
    "테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) 회고\n",
    "\n",
    "- 이번 프로젝트에서 **어려웠던 점,**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 루브릭 평가 지표를 맞추기 위해 **시도한 것들**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**\n",
    "\n",
    "\n",
    "- **자기 다짐**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64f60db5d244171661447d9f571ca5a7e4bf0bab2148f20540b6d51562bb9ca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
